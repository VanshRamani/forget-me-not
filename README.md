# Bayesian PAC Unlearning Framework

A modular implementation of the Bayesian PAC (Probably Approximately Correct) Unlearning framework for machine learning model unlearning.

## Overview

This framework implements a principled approach to machine unlearning that:
- **Forgets** specified data (forget region Xâ‚) 
- **Retains** model utility on remaining data (retain region Xâ‚€)
- Provides **theoretical guarantees** via PAC conditions (Îµ-MC and Î»-UC)

## Framework Components

### 1. Synthetic Dataset Generation (`src/synthetic.py`)

Create datasets with clearly defined forget/retain regions:

```python
from src.synthetic import generate_moon_dataset, TargetConcept, partition_data

# Generate data
X, y = generate_moon_dataset(n_samples=500)

# Define forget region (e.g., spatial region)
concept = TargetConcept.spatial_region(x_min=-0.5, x_max=1.5, y_min=-0.5, y_max=0.5)

# Partition into retain/forget
X_retain, X_forget, partition = partition_data(X, y, concept)
```

**Supported datasets:**
- `generate_moon_dataset()` - Two interleaving moons
- `generate_circles_dataset()` - Concentric circles
- `generate_blobs_dataset()` - Gaussian blobs
- `generate_gaussian_mixture()` - Custom Gaussian mixtures

**Supported concepts:**
- `TargetConcept.spatial_region()` - Rectangular region
- `TargetConcept.radial()` - Circular region
- `TargetConcept.halfspace()` - Linear boundary
- `TargetConcept.class_based()` - Class-based forgetting

### 2. Bayesian Unlearning (`src/bayesian.py`)

Modular components for Bayesian inference:

#### Priors
- `DirichletProcessPrior` - Flexible Dirichlet Process prior
- `GaussianPrior` - Gaussian prior on mixture components

#### Distributions
- `GaussianMixtureDistribution` - Gaussian mixture model
- `KDEDistribution` - Kernel density estimate

#### Likelihoods
- `RetainLikelihood` - Standard likelihood: L(Dâ‚€|P) = âˆ P(x)
- `ForgetLikelihood` - Tilted likelihood: L(Dâ‚|P) âˆ [âˆ P(x)]^(-Î»)
- `JointLikelihood` - Combined retain + forget

#### Main Algorithm
```python
from src.bayesian import BayesianUnlearning, GaussianPrior

# Setup prior
prior = GaussianPrior(mean_prior_mean=data_mean, mean_prior_cov=data_cov)

# Create unlearner
unlearner = BayesianUnlearning(prior=prior, lambda_hyper=2.0)

# Fit (performs Bayesian inference)
unlearner.fit(X_retain, X_forget, n_posterior_samples=50)

# Get unlearned model
Q_unlearned = unlearner.get_posterior_predictive()

# Evaluate
metrics = unlearner.evaluate(P_ideal, X_forget)
# Returns: {'epsilon': ..., 'lambda': ..., 'satisfies_pac': ...}
```

### 3. Unlearning Metrics

#### PAC Framework Metrics

**Îµ-Matching Condition (Îµ-MC):** Measures utility
- `d_TV(Q, Pâ‚€) â‰¤ Îµ` - Output Q is close to ideal distribution Pâ‚€

**Î»-Unlearning Condition (Î»-UC):** Measures forgetting
- `Q(Xâ‚) â‰¤ Î»` - Low probability mass in forget region

**Key Theorem:** Îµ-MC implies Î»-UC with Î» = Îµ

#### Additional Standard Unlearning Metrics

The framework also computes commonly used distribution distance metrics:

**Information-Theoretic:**
- **KL Divergence** `D_KL(Pâ‚€||Q)` - Asymmetric measure of how Pâ‚€ diverges from Q
- **JS Divergence** - Symmetric version of KL, bounded in [0, log(2)] â‰ˆ [0, 0.693]

**Geometric Distances:**
- **Hellinger Distance** - Symmetric, bounded in [0, 1]
- **Wasserstein Distance (Wâ‚)** - Earth Mover's Distance (sliced for multivariate)
- **Chi-Squared Distance** - Weighted difference measure

All metrics are computed automatically during evaluation and can be used to comprehensively assess unlearning quality.

## Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Or manually:
pip install numpy scipy scikit-learn matplotlib
```

## Usage

### Quick Start

Run the demo script:

```bash
python demo.py
```

This will:
1. Generate synthetic dataset with forget/retain regions
2. Fit ideal retain distribution Pâ‚€
3. Perform Bayesian unlearning with different Î»_hyper values
4. Evaluate PAC metrics (Îµ and Î»)
5. Visualize results

### Custom Experiment

```python
from src.synthetic import generate_moon_dataset, TargetConcept, partition_data
from src.bayesian import BayesianUnlearning, GaussianPrior, fit_distribution_to_data

# 1. Generate data
X, y = generate_moon_dataset(n_samples=800)
concept = TargetConcept.radial(center=(1.0, 0.0), radius=0.7)
X_retain, X_forget, _ = partition_data(X, y, concept)

# 2. Fit ideal retain distribution
P_ideal = fit_distribution_to_data(X_retain, method='gmm', n_components=5)

# 3. Setup and run unlearning
prior = GaussianPrior(mean_prior_mean=X_retain.mean(axis=0), 
                     mean_prior_cov=np.cov(X_retain.T))
unlearner = BayesianUnlearning(prior=prior, lambda_hyper=2.0)
unlearner.fit(X_retain, X_forget, n_posterior_samples=50)

# 4. Get results
Q_unlearned = unlearner.get_posterior_predictive()
metrics = unlearner.evaluate(P_ideal, X_forget)
print(f"Îµ = {metrics['epsilon']:.4f}, Î» = {metrics['lambda']:.4f}")
```

## Modular Design

The framework is designed for easy customization:

### Change Prior
```python
# Instead of GaussianPrior, use DirichletProcess
from src.bayesian import DirichletProcessPrior, GaussianMixtureDistribution

base_measure = fit_distribution_to_data(X_retain, method='gmm')
prior = DirichletProcessPrior(concentration=1.0, base_measure=base_measure)
```

### Change Likelihood
```python
# Customize forgetting strength
from src.bayesian import ForgetLikelihood

forget_lik = ForgetLikelihood(lambda_hyper=5.0)  # Stronger forgetting
```

### Change Distribution Representation
```python
# Use KDE instead of GMM
from src.bayesian import KDEDistribution

P_ideal = KDEDistribution(X_retain)
```

### Change Target Concept
```python
# Custom concept function
def my_concept(X):
    # Forget points where x1 > x2
    return (X[:, 0] > X[:, 1]).astype(int)

concept = TargetConcept(my_concept, name="custom")
```

## Theory

### PAC Unlearning Framework

Given:
- Instance space X
- Target concept h: X â†’ {0,1} (0=retain, 1=forget)
- Ideal retain distribution Pâ‚€
- Output distribution Q (from unlearning algorithm)

Objectives:
1. **Îµ-MC:** d(Q, Pâ‚€) â‰¤ Îµ (maintain utility)
2. **Î»-UC:** Q(Xâ‚) â‰¤ Î» (achieve forgetting)

### Bayesian Formulation

Posterior inference:
```
Ï€(P | Dâ‚€, Dâ‚) âˆ L(Dâ‚€|P) Ã— L(Dâ‚|P) Ã— Ï€(P)
                âˆ [âˆ P(x)]      Ã— [âˆ P(x)]^(-Î»_hyper) Ã— Ï€(P)
                   xâˆˆDâ‚€           xâˆˆDâ‚
```

Output: Q_Bayes(x) = ğ”¼[P(x) | Dâ‚€, Dâ‚]

## File Structure

```
PAC-Bayesian/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ synthetic.py      # Dataset generation
â”‚   â””â”€â”€ bayesian.py       # Bayesian unlearning framework
â”œâ”€â”€ demo.py               # Demonstration script
â”œâ”€â”€ requirements.txt      # Dependencies
â”œâ”€â”€ Framework.md          # Theoretical framework
â””â”€â”€ README.md            # This file
```

## Key Hyperparameters

- **Î»_hyper**: Forgetting strength (higher = stronger forgetting, but may reduce utility)
- **n_posterior_samples**: Number of samples for posterior approximation
- **n_components**: Number of mixture components for distributions
- **concentration** (DP prior): Controls prior flexibility

## Examples

See `demo.py` for:
- Basic unlearning experiment
- Comparison of different Î»_hyper values
- Visualization of results

## License

MIT License

## Contributing

The modular design encourages experimentation. Feel free to:
- Add new prior distributions
- Implement alternative likelihood functions
- Create new target concepts
- Extend to higher dimensions or different data types
